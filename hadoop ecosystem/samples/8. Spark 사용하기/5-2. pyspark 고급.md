---
style: |
  img {
    display: block;
    float: none;
    margin-left: auto;
    margin-right: auto;
  }
marp: true
paginate: true
---
# Create SparkContext
```python
%spark.pyspark

import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("yarn") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()
                    
print(spark.sparkContext)
print("Spark App Name : "+ spark.sparkContext.appName)
```
---
## [PySpark transform()](https://sparkbyexamples.com/pyspark/pyspark-transform-function/)
```python
# Prepare Data
simpleData = (("Java",4000,5), \
    ("Python", 4600,10),  \
    ("Scala", 4100,15),   \
    ("Scala", 4500,15),   \
    ("PHP", 3000,20),  \
  )
columns= ["CourseName", "fee", "discount"]

# Create DataFrame
df = spark.createDataFrame(data = simpleData, schema = columns)
df.printSchema()
df.show(truncate=False)
```
---
### 단계1: Create Custom Functions
```python
# Custom transformation 1
from pyspark.sql.functions import upper
def to_upper_str_columns(df):
    return df.withColumn("CourseName",upper(df.CourseName))

# Custom transformation 2
def apply_discount(df):
    return df.withColumn("discounted_fee",  \
             df.fee - (df.fee * df.discount) / 100)
```
### 단계2: PySpark Apply DataFrame.transform()
```python
df.transform(to_upper_str_columns) \
        .transform(apply_discount).show(truncate=False)
```
---
### 단계3: transform with lambda
```python
# Create DataFrame with Array
data = [
 ("James,,Smith",["Java","Scala","C++"],["Spark","Java"]),
 ("Michael,Rose,",["Spark","Java","C++"],["Spark","Java"]),
 ("Robert,,Williams",["CSharp","VB"],["Spark","Python"])
]
df = spark.createDataFrame(data=data,schema=["Name","Languages1","Languages2"])
df.printSchema()
df.show()

# using transform() function
from pyspark.sql.functions import upper
from pyspark.sql.functions import transform
df.select(transform("Languages1", lambda x: upper(x)).alias("languages1")) \
  .show()
```
---
## [PySpark map()](https://sparkbyexamples.com/pyspark/pyspark-map-transformation/)
```python
data = [('James','Smith','M',30),
  ('Anna','Rose','F',41),
  ('Robert','Williams','M',62), 
]

columns = ["firstname","lastname","gender","salary"]
df = spark.createDataFrame(data=data, schema = columns)
df.show()
```
---
### 단계1: 예제
```python
rdd2=df.rdd.map(lambda x: 
    (x["firstname"]+","+x["lastname"],x["gender"],x["salary"]*2)
    ) 
df2=rdd2.toDF(["name","gender","new_salary"]   )
df2.show()
```
### 단계2: 예제
```python
def func1(x):
    firstName=x.firstname
    lastName=x.lastname
    name=firstName+","+lastName
    gender=x.gender.lower()
    salary=x.salary*2
    return (name,gender,salary)

rdd2=df.rdd.map(lambda x: func1(x))
df2.show()
```
---
## [PySpark foreach()](https://sparkbyexamples.com/pyspark/pyspark-foreach-usage-with-examples/)




---
### 단계1: 
```python

```






















