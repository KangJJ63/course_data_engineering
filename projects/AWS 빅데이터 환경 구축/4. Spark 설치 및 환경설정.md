---
style: |
  img {
    display: block;
    float: none;
    margin-left: auto;
    margin-right: auto;
  }
marp: true
paginate: true
---
# [AWS EC2 인스턴스 배포](https://www.youtube.com/watch?v=QQUKRVmdGVc&list=PLJlUnZ1kDbt7X2C4ntIYHmphNDIc5wN8J&index=5)
- [관련 문서](https://biggongjam.notion.site/4-Spark-2c341ddc8715411484cb2f0254b60126)

---
### 단계1: Spark 설치 
- Apache Spark 3.2.1 설치 및 압축 해제
```shell
# EC2 Ubuntu terminal

# 설치 관리용 디렉토리 이동
cd /install_dir
# Spark 3.2.1 설치
sudo wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
# Spark 3.2.1 압축 해제
sudo tar -xzvf spark-3.2.1-bin-hadoop3.2.tgz -C /usr/local
# Spark 디렉토리 이름 변경
sudo mv /usr/local/spark-3.2.1-bin-hadoop3.2 /usr/local/spark
```
---
### 단계2: Python & PySpark 설치 
```shell
# EC2 Ubuntu terminal

# Python 설치
sudo apt-get install -y python3-pip
# Python 버전 확인
python3 -V
# PySpark 설치
sudo pip3 install pyspark findspark
```
---
### 단계3: Spark 환경설정
```shell
# EC2 Ubuntu terminal

# Hadoop 시스템 환경변수 설정
sudo vim /etc/environment

# 아래 내용 추가 후 저장
PATH 뒤에 ":/usr/local/spark/bin" 추가
PATH 뒤에 ":/usr/local/spark/sbin" 추가
SPARK_HOME="/usr/local/spark"

# 시스템 환경변수 활성화
source /etc/environment

#  Spark 사용자 환경변수 설정
echo 'export SPARK_HOME=/usr/local/spark' >> ~/.bashrc

# 사용자 환경변수 활성화
source ~/.bashrc

# 환경변수 확인 
env | grep SPARK
```

---
### 단계4: spark-env.sh
```shell
# EC2 Ubuntu terminal

# spark-env.sh 파일 카피
cd $SPARK_HOME/conf
sudo cp spark-env.sh.template spark-env.sh

# spark-env.sh 파일 편집
sudo vim spark-env.sh

# 아래 내용 수정 후 저장
export SPARK_HOME=/usr/local/spark
export SPARK_CONF_DIR=/usr/local/spark/conf
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export SPARK_MASTER_WEBUI_PORT=18080
```

---
### 단계5: spark-defaults.conf
```shell
# EC2 Ubuntu terminal

# Spark spark-defaults.conf.template 파일 복사
sudo cp /usr/local/spark/conf/spark-defaults.conf.template /usr/local/spark/conf/spark-defaults.conf

# Spark spark-defaults.conf 파일 설정
sudo vim /usr/local/spark/conf/spark-defaults.conf

# 아래 설정 후 저장 
# 클러스터 매니저 정보
spark.master              yarn
# 스파크 이벤트 로그 수행 유무
# true시 spark.eventLog.dir에 로깅 경로 지정해야합니다 - 스파크 UI에서 확인 가능합니다.
spark.eventLog.enabled    true
# 스파크 이벤트 로그 저장 경로
spark.eventLog.dir        /usr/local/spark/logs
```

---
### 단계6: Spark logs 디렉토리 생성
```shell
# EC2 Ubuntu terminal

sudo mkdir -p /usr/local/spark/logs && sudo chown -R $USER:$USER /usr/local/spark/
```
### 단계7: workers 파일 편집
```shell
# EC2 Ubuntu terminal

# Spark workers 파일 생성
sudo cp /usr/local/spark/conf/workers.template /usr/local/spark/conf/workers

# Spark workers 파일 설정
sudo vim /usr/local/spark/conf/workers

# 아래 설정 후 저장(단, localhost는 주석 처리한다.)
dn1
dn2
dn3
```
---
### 단계8: Python 환경설정 
```shell
# EC2 Ubuntu terminal

# 시스템 환경변수 편집
sudo vim /etc/environment

# 아래 내용 추가 후 저장
PATH 뒤에 ":/usr/bin/python3" 추가

# 시스템 환경변수 활성화
source /etc/environment

#  Python & PySpark 사용자 환경변수 설정
sudo echo 'export PYTHONPATH=/usr/bin/python3' >> ~/.bashrc
sudo echo 'export PYSPARK_PYTHON=/usr/bin/python3' >> ~/.bashrc

# 사용자 환경변수 활성화
source ~/.bashrc
```





